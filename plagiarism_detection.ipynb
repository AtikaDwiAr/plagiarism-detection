{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5312c3b",
   "metadata": {},
   "source": [
    "# Sistem Deteksi Plagiarisme\n",
    "\n",
    "Notebook ini berisi implementasi sistem deteksi plagiarisme berdasarkan proposal penelitian.\n",
    "\n",
    "## Tim:\n",
    "- Nugroho Adi Susanto\n",
    "- (Anggota tim lainnya)\n",
    "\n",
    "## Latar Belakang\n",
    "Deteksi plagiarisme merupakan proses identifikasi kesamaan konten yang tidak wajar antara dua atau lebih dokumen. Sistem deteksi plagiarisme diperlukan untuk menjaga integritas akademik dan mencegah praktik plagiarisme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a0eeb",
   "metadata": {},
   "source": [
    "## Import Library\n",
    "\n",
    "Import library yang diperlukan untuk pengolahan teks, ekstraksi fitur, dan visualisasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020dbcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using regular tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nugroho-adi-\n",
      "[nltk_data]     susanto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nugroho-adi-\n",
      "[nltk_data]     susanto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import library standar\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup tqdm with fallback - force regular tqdm to avoid ipywidgets issues\n",
    "from tqdm import tqdm\n",
    "print(\"Using regular tqdm\")\n",
    "\n",
    "# Library untuk pengolahan teks\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Library untuk ekstraksi fitur dan pembandingan\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import difflib\n",
    "\n",
    "# Download resources NLTK yang dibutuhkan\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c8e81",
   "metadata": {},
   "source": [
    "## Pengaturan Awal\n",
    "\n",
    "Menyiapkan fungsi-fungsi dasar untuk pemrosesan teks dan deteksi plagiarisme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119a9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi stemmer Bahasa Indonesia\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Definisikan stopwords untuk Bahasa Indonesia\n",
    "indo_stopwords = set(stopwords.words('indonesian'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess teks dengan tokenisasi, menghilangkan stopwords, dan stemming\"\"\"\n",
    "    # Lowercase dan hapus karakter khusus\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    \n",
    "    # Tokenisasi kata\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Hapus stopwords\n",
    "    tokens = [word for word in tokens if word not in indo_stopwords]\n",
    "    \n",
    "    # Stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13734642",
   "metadata": {},
   "source": [
    "## Pembuatan Corpus dan Pembacaan Dataset\n",
    "\n",
    "Pada bagian ini, kita akan membaca dataset dokumen yang akan dianalisis untuk deteksi plagiarisme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4076ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 200 documents (100 source, 100 suspicious).\n",
      "Source documents:\n",
      "  [Source 1] preprocessed_source-document00086.txt\n",
      "  [Source 2] preprocessed_source-document00087.txt\n",
      "  [Source 3] preprocessed_source-document00088.txt\n",
      "  [Source 4] preprocessed_source-document00089.txt\n",
      "  [Source 5] preprocessed_source-document00090.txt\n",
      "  [Source 6] preprocessed_source-document00091.txt\n",
      "  [Source 7] preprocessed_source-document00092.txt\n",
      "  [Source 8] preprocessed_source-document00093.txt\n",
      "  [Source 9] preprocessed_source-document00094.txt\n",
      "  [Source 10] preprocessed_source-document00095.txt\n",
      "  [Source 11] preprocessed_source-document00096.txt\n",
      "  [Source 12] preprocessed_source-document00097.txt\n",
      "  [Source 13] preprocessed_source-document00098.txt\n",
      "  [Source 14] preprocessed_source-document00099.txt\n",
      "  [Source 15] preprocessed_source-document00100.txt\n",
      "  [Source 16] preprocessed_source-document00051.txt\n",
      "  [Source 17] preprocessed_source-document00052.txt\n",
      "  [Source 18] preprocessed_source-document00053.txt\n",
      "  [Source 19] preprocessed_source-document00054.txt\n",
      "  [Source 20] preprocessed_source-document00055.txt\n",
      "  [Source 21] preprocessed_source-document00056.txt\n",
      "  [Source 22] preprocessed_source-document00057.txt\n",
      "  [Source 23] preprocessed_source-document00058.txt\n",
      "  [Source 24] preprocessed_source-document00059.txt\n",
      "  [Source 25] preprocessed_source-document00060.txt\n",
      "  [Source 26] preprocessed_source-document00061.txt\n",
      "  [Source 27] preprocessed_source-document00062.txt\n",
      "  [Source 28] preprocessed_source-document00063.txt\n",
      "  [Source 29] preprocessed_source-document00064.txt\n",
      "  [Source 30] preprocessed_source-document00065.txt\n",
      "  [Source 31] preprocessed_source-document00066.txt\n",
      "  [Source 32] preprocessed_source-document00067.txt\n",
      "  [Source 33] preprocessed_source-document00068.txt\n",
      "  [Source 34] preprocessed_source-document00069.txt\n",
      "  [Source 35] preprocessed_source-document00070.txt\n",
      "  [Source 36] preprocessed_source-document00071.txt\n",
      "  [Source 37] preprocessed_source-document00072.txt\n",
      "  [Source 38] preprocessed_source-document00019.txt\n",
      "  [Source 39] preprocessed_source-document00020.txt\n",
      "  [Source 40] preprocessed_source-document00021.txt\n",
      "  [Source 41] preprocessed_source-document00022.txt\n",
      "  [Source 42] preprocessed_source-document00023.txt\n",
      "  [Source 43] preprocessed_source-document00024.txt\n",
      "  [Source 44] preprocessed_source-document00025.txt\n",
      "  [Source 45] preprocessed_source-document00026.txt\n",
      "  [Source 46] preprocessed_source-document00027.txt\n",
      "  [Source 47] preprocessed_source-document00028.txt\n",
      "  [Source 48] preprocessed_source-document00029.txt\n",
      "  [Source 49] preprocessed_source-document00030.txt\n",
      "  [Source 50] preprocessed_source-document00031.txt\n",
      "  [Source 51] preprocessed_source-document00032.txt\n",
      "  [Source 52] preprocessed_source-document00033.txt\n",
      "  [Source 53] preprocessed_source-document00034.txt\n",
      "  [Source 54] preprocessed_source-document00035.txt\n",
      "  [Source 55] preprocessed_source-document00036.txt\n",
      "  [Source 56] preprocessed_source-document00037.txt\n",
      "  [Source 57] preprocessed_source-document00074.txt\n",
      "  [Source 58] preprocessed_source-document00075.txt\n",
      "  [Source 59] preprocessed_source-document00076.txt\n",
      "  [Source 60] preprocessed_source-document00077.txt\n",
      "  [Source 61] preprocessed_source-document00078.txt\n",
      "  [Source 62] preprocessed_source-document00079.txt\n",
      "  [Source 63] preprocessed_source-document00080.txt\n",
      "  [Source 64] preprocessed_source-document00081.txt\n",
      "  [Source 65] preprocessed_source-document00082.txt\n",
      "  [Source 66] preprocessed_source-document00083.txt\n",
      "  [Source 67] preprocessed_source-document00084.txt\n",
      "  [Source 68] preprocessed_source-document00018.txt\n",
      "  [Source 69] preprocessed_source-document00038.txt\n",
      "  [Source 70] preprocessed_source-document00050.txt\n",
      "  [Source 71] preprocessed_source-document00073.txt\n",
      "  [Source 72] preprocessed_source-document00085.txt\n",
      "  [Source 73] preprocessed_source-document00039.txt\n",
      "  [Source 74] preprocessed_source-document00040.txt\n",
      "  [Source 75] preprocessed_source-document00041.txt\n",
      "  [Source 76] preprocessed_source-document00042.txt\n",
      "  [Source 77] preprocessed_source-document00043.txt\n",
      "  [Source 78] preprocessed_source-document00044.txt\n",
      "  [Source 79] preprocessed_source-document00045.txt\n",
      "  [Source 80] preprocessed_source-document00046.txt\n",
      "  [Source 81] preprocessed_source-document00047.txt\n",
      "  [Source 82] preprocessed_source-document00048.txt\n",
      "  [Source 83] preprocessed_source-document00049.txt\n",
      "  [Source 84] preprocessed_source-document00001.txt\n",
      "  [Source 85] preprocessed_source-document00002.txt\n",
      "  [Source 86] preprocessed_source-document00003.txt\n",
      "  [Source 87] preprocessed_source-document00004.txt\n",
      "  [Source 88] preprocessed_source-document00005.txt\n",
      "  [Source 89] preprocessed_source-document00006.txt\n",
      "  [Source 90] preprocessed_source-document00007.txt\n",
      "  [Source 91] preprocessed_source-document00008.txt\n",
      "  [Source 92] preprocessed_source-document00009.txt\n",
      "  [Source 93] preprocessed_source-document00010.txt\n",
      "  [Source 94] preprocessed_source-document00011.txt\n",
      "  [Source 95] preprocessed_source-document00012.txt\n",
      "  [Source 96] preprocessed_source-document00013.txt\n",
      "  [Source 97] preprocessed_source-document00014.txt\n",
      "  [Source 98] preprocessed_source-document00015.txt\n",
      "  [Source 99] preprocessed_source-document00016.txt\n",
      "  [Source 100] preprocessed_source-document00017.txt\n",
      "Suspicious documents:\n",
      "  [Suspicious 1] preprocessed_suspicious-document00080.txt\n",
      "  [Suspicious 2] preprocessed_suspicious-document00081.txt\n",
      "  [Suspicious 3] preprocessed_suspicious-document00082.txt\n",
      "  [Suspicious 4] preprocessed_suspicious-document00083.txt\n",
      "  [Suspicious 5] preprocessed_suspicious-document00084.txt\n",
      "  [Suspicious 6] preprocessed_suspicious-document00085.txt\n",
      "  [Suspicious 7] preprocessed_suspicious-document00086.txt\n",
      "  [Suspicious 8] preprocessed_suspicious-document00087.txt\n",
      "  [Suspicious 9] preprocessed_suspicious-document00088.txt\n",
      "  [Suspicious 10] preprocessed_suspicious-document00089.txt\n",
      "  [Suspicious 11] preprocessed_suspicious-document00090.txt\n",
      "  [Suspicious 12] preprocessed_suspicious-document00091.txt\n",
      "  [Suspicious 13] preprocessed_suspicious-document00092.txt\n",
      "  [Suspicious 14] preprocessed_suspicious-document00093.txt\n",
      "  [Suspicious 15] preprocessed_suspicious-document00094.txt\n",
      "  [Suspicious 16] preprocessed_suspicious-document00095.txt\n",
      "  [Suspicious 17] preprocessed_suspicious-document00096.txt\n",
      "  [Suspicious 18] preprocessed_suspicious-document00097.txt\n",
      "  [Suspicious 19] preprocessed_suspicious-document00098.txt\n",
      "  [Suspicious 20] preprocessed_suspicious-document00099.txt\n",
      "  [Suspicious 21] preprocessed_suspicious-document00100.txt\n",
      "  [Suspicious 22] preprocessed_suspicious-document00032.txt\n",
      "  [Suspicious 23] preprocessed_suspicious-document00033.txt\n",
      "  [Suspicious 24] preprocessed_suspicious-document00034.txt\n",
      "  [Suspicious 25] preprocessed_suspicious-document00035.txt\n",
      "  [Suspicious 26] preprocessed_suspicious-document00036.txt\n",
      "  [Suspicious 27] preprocessed_suspicious-document00037.txt\n",
      "  [Suspicious 28] preprocessed_suspicious-document00038.txt\n",
      "  [Suspicious 29] preprocessed_suspicious-document00039.txt\n",
      "  [Suspicious 30] preprocessed_suspicious-document00040.txt\n",
      "  [Suspicious 31] preprocessed_suspicious-document00041.txt\n",
      "  [Suspicious 32] preprocessed_suspicious-document00042.txt\n",
      "  [Suspicious 33] preprocessed_suspicious-document00043.txt\n",
      "  [Suspicious 34] preprocessed_suspicious-document00044.txt\n",
      "  [Suspicious 35] preprocessed_suspicious-document00045.txt\n",
      "  [Suspicious 36] preprocessed_suspicious-document00046.txt\n",
      "  [Suspicious 37] preprocessed_suspicious-document00060.txt\n",
      "  [Suspicious 38] preprocessed_suspicious-document00061.txt\n",
      "  [Suspicious 39] preprocessed_suspicious-document00062.txt\n",
      "  [Suspicious 40] preprocessed_suspicious-document00063.txt\n",
      "  [Suspicious 41] preprocessed_suspicious-document00064.txt\n",
      "  [Suspicious 42] preprocessed_suspicious-document00065.txt\n",
      "  [Suspicious 43] preprocessed_suspicious-document00066.txt\n",
      "  [Suspicious 44] preprocessed_suspicious-document00067.txt\n",
      "  [Suspicious 45] preprocessed_suspicious-document00068.txt\n",
      "  [Suspicious 46] preprocessed_suspicious-document00069.txt\n",
      "  [Suspicious 47] preprocessed_suspicious-document00070.txt\n",
      "  [Suspicious 48] preprocessed_suspicious-document00071.txt\n",
      "  [Suspicious 49] preprocessed_suspicious-document00072.txt\n",
      "  [Suspicious 50] preprocessed_suspicious-document00073.txt\n",
      "  [Suspicious 51] preprocessed_suspicious-document00074.txt\n",
      "  [Suspicious 52] preprocessed_suspicious-document00075.txt\n",
      "  [Suspicious 53] preprocessed_suspicious-document00076.txt\n",
      "  [Suspicious 54] preprocessed_suspicious-document00077.txt\n",
      "  [Suspicious 55] preprocessed_suspicious-document00078.txt\n",
      "  [Suspicious 56] preprocessed_suspicious-document00020.txt\n",
      "  [Suspicious 57] preprocessed_suspicious-document00021.txt\n",
      "  [Suspicious 58] preprocessed_suspicious-document00022.txt\n",
      "  [Suspicious 59] preprocessed_suspicious-document00023.txt\n",
      "  [Suspicious 60] preprocessed_suspicious-document00024.txt\n",
      "  [Suspicious 61] preprocessed_suspicious-document00025.txt\n",
      "  [Suspicious 62] preprocessed_suspicious-document00026.txt\n",
      "  [Suspicious 63] preprocessed_suspicious-document00027.txt\n",
      "  [Suspicious 64] preprocessed_suspicious-document00028.txt\n",
      "  [Suspicious 65] preprocessed_suspicious-document00029.txt\n",
      "  [Suspicious 66] preprocessed_suspicious-document00030.txt\n",
      "  [Suspicious 67] preprocessed_suspicious-document00019.txt\n",
      "  [Suspicious 68] preprocessed_suspicious-document00031.txt\n",
      "  [Suspicious 69] preprocessed_suspicious-document00047.txt\n",
      "  [Suspicious 70] preprocessed_suspicious-document00059.txt\n",
      "  [Suspicious 71] preprocessed_suspicious-document00079.txt\n",
      "  [Suspicious 72] preprocessed_suspicious-document00048.txt\n",
      "  [Suspicious 73] preprocessed_suspicious-document00049.txt\n",
      "  [Suspicious 74] preprocessed_suspicious-document00050.txt\n",
      "  [Suspicious 75] preprocessed_suspicious-document00051.txt\n",
      "  [Suspicious 76] preprocessed_suspicious-document00052.txt\n",
      "  [Suspicious 77] preprocessed_suspicious-document00053.txt\n",
      "  [Suspicious 78] preprocessed_suspicious-document00054.txt\n",
      "  [Suspicious 79] preprocessed_suspicious-document00055.txt\n",
      "  [Suspicious 80] preprocessed_suspicious-document00056.txt\n",
      "  [Suspicious 81] preprocessed_suspicious-document00057.txt\n",
      "  [Suspicious 82] preprocessed_suspicious-document00058.txt\n",
      "  [Suspicious 83] preprocessed_suspicious-document00001.txt\n",
      "  [Suspicious 84] preprocessed_suspicious-document00002.txt\n",
      "  [Suspicious 85] preprocessed_suspicious-document00003.txt\n",
      "  [Suspicious 86] preprocessed_suspicious-document00004.txt\n",
      "  [Suspicious 87] preprocessed_suspicious-document00005.txt\n",
      "  [Suspicious 88] preprocessed_suspicious-document00006.txt\n",
      "  [Suspicious 89] preprocessed_suspicious-document00007.txt\n",
      "  [Suspicious 90] preprocessed_suspicious-document00008.txt\n",
      "  [Suspicious 91] preprocessed_suspicious-document00009.txt\n",
      "  [Suspicious 92] preprocessed_suspicious-document00010.txt\n",
      "  [Suspicious 93] preprocessed_suspicious-document00011.txt\n",
      "  [Suspicious 94] preprocessed_suspicious-document00012.txt\n",
      "  [Suspicious 95] preprocessed_suspicious-document00013.txt\n",
      "  [Suspicious 96] preprocessed_suspicious-document00014.txt\n",
      "  [Suspicious 97] preprocessed_suspicious-document00015.txt\n",
      "  [Suspicious 98] preprocessed_suspicious-document00016.txt\n",
      "  [Suspicious 99] preprocessed_suspicious-document00017.txt\n",
      "  [Suspicious 100] preprocessed_suspicious-document00018.txt\n"
     ]
    }
   ],
   "source": [
    "# Definisikan path ke direktori yang berisi dokumen\n",
    "data_dir1 = \"../plagiarism-detection/preprocessed_data/source\"\n",
    "data_dir2 = \"../plagiarism-detection/preprocessed_data/suspicious\"\n",
    "# Fungsi untuk membaca semua dokumen dalam direktori\n",
    "def read_documents(directory):\n",
    "    documents = {}\n",
    "    \n",
    "    # Cek apakah direktori tersedia\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory {directory} not found. Creating it...\")\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        print(f\"Please place your document files in {os.path.abspath(directory)}\")\n",
    "        return {}\n",
    "    \n",
    "    # Baca semua file teks di direktori\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    documents[filename] = content\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Membaca dokumen dari direktori\n",
    "documents = {\"source\" : read_documents(data_dir1), \"suspicious\" : read_documents(data_dir2)}\n",
    "\n",
    "# Tampilkan informasi dokumen yang berhasil dibaca\n",
    "if documents:\n",
    "    num_source = len(documents[\"source\"])\n",
    "    num_suspicious = len(documents[\"suspicious\"])\n",
    "    total_docs = num_source + num_suspicious\n",
    "    print(f\"Successfully loaded {total_docs} documents ({num_source} source, {num_suspicious} suspicious).\")\n",
    "    print(\"Source documents:\")\n",
    "    for idx, filename in enumerate(documents[\"source\"].keys(), 1):\n",
    "        print(f\"  [Source {idx}] {filename}\")\n",
    "    print(\"Suspicious documents:\")\n",
    "    for idx, filename in enumerate(documents[\"suspicious\"].keys(), 1):\n",
    "        print(f\"  [Suspicious {idx}] {filename}\")\n",
    "else:\n",
    "    print(\"No documents found. Please add .txt files to the data directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f514203",
   "metadata": {},
   "source": [
    "## Ekstraksi Fitur\n",
    "\n",
    "Mengimplementasikan berbagai metode ekstraksi fitur untuk deteksi plagiarisme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d052eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlagiarismDetector:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    def preprocess_documents(self, documents):\n",
    "        \"\"\"Preprocess semua dokumen\"\"\"\n",
    "        processed_docs = {}\n",
    "        for filename, text in documents.items():\n",
    "            processed_docs[filename] = preprocess_text(text)\n",
    "        return processed_docs\n",
    "    \n",
    "    def compute_cosine_similarity(self, processed_docs):\n",
    "        \"\"\"Menghitung similaritas kosinus antara dokumen\"\"\"\n",
    "        docs_list = list(processed_docs.values())\n",
    "        filenames = list(processed_docs.keys())\n",
    "        \n",
    "        # Transformasi teks menjadi vektor TF-IDF\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(docs_list)\n",
    "        \n",
    "        # Hitung similaritas kosinus\n",
    "        cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "        \n",
    "        # Buat DataFrame untuk visualisasi\n",
    "        similarity_df = pd.DataFrame(cosine_similarities, index=filenames, columns=filenames)\n",
    "        \n",
    "        return similarity_df\n",
    "    \n",
    "    def extract_n_grams(self, text, n=3):\n",
    "        \"\"\"Ekstraksi n-gram dari teks\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        n_grams = []\n",
    "        \n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            n_grams.append(' '.join(tokens[i:i+n]))\n",
    "            \n",
    "        return n_grams\n",
    "    \n",
    "    def compute_n_gram_similarity(self, doc1, doc2, n=3):\n",
    "        \"\"\"Hitung similaritas berdasarkan n-gram\"\"\"\n",
    "        ngrams1 = set(self.extract_n_grams(doc1, n))\n",
    "        ngrams2 = set(self.extract_n_grams(doc2, n))\n",
    "        \n",
    "        # Jaccard similarity\n",
    "        intersection = len(ngrams1.intersection(ngrams2))\n",
    "        union = len(ngrams1.union(ngrams2))\n",
    "        \n",
    "        if union == 0:\n",
    "            return 0\n",
    "        \n",
    "        return intersection / union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fed370",
   "metadata": {},
   "source": [
    "## Menjalankan Deteksi Plagiarisme dengan TF-IDF dan Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f76de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan 6 dari 8 CPU untuk pemrosesan paralel\n",
      "Melakukan preprocessing dokumen sumber...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing source docs: 100%|██████████| 100/100 [1:14:42<00:00, 44.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melakukan preprocessing dokumen yang dicurigai...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing suspicious docs:  12%|█▏        | 12/100 [00:42<05:13,  3.56s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMelakukan preprocessing dokumen yang dicurigai...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m suspicious_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(suspicious_docs\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m---> 36\u001b[0m suspicious_processed_items \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiprocessing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_document\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_item\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc_item\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msuspicious_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessing suspicious docs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m suspicious_processed_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(suspicious_processed_items)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Gabungkan semua dokumen terproses untuk analisis cross-similarity\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import library untuk pemrosesan paralel\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# tqdm is already imported in the setup cell\n",
    "\n",
    "# Inisialisasi detektor\n",
    "detector = PlagiarismDetector()\n",
    "\n",
    "# Menentukan jumlah CPU yang akan digunakan (semua kecuali 2)\n",
    "n_jobs = max(1, multiprocessing.cpu_count() - 2)\n",
    "print(f\"Menggunakan {n_jobs} dari {multiprocessing.cpu_count()} CPU untuk pemrosesan paralel\")\n",
    "\n",
    "# Proses untuk dokumen sumber dan dokumen yang dicurigai secara terpisah\n",
    "source_docs = documents[\"source\"]\n",
    "suspicious_docs = documents[\"suspicious\"]\n",
    "\n",
    "if source_docs and suspicious_docs:\n",
    "    # Fungsi untuk preprocessing dokumen secara paralel\n",
    "    def preprocess_document(doc_tuple):\n",
    "        filename, content = doc_tuple\n",
    "        processed = preprocess_text(content)\n",
    "        return filename, processed\n",
    "    \n",
    "    # Preprocess dokumen sumber dengan pemrosesan paralel\n",
    "    print(\"Melakukan preprocessing dokumen sumber...\")\n",
    "    source_items = list(source_docs.items())\n",
    "    source_processed_items = Parallel(n_jobs=n_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(preprocess_document)(doc_item) for doc_item in tqdm(source_items, desc=\"Processing source docs\")\n",
    "    )\n",
    "    source_processed_docs = dict(source_processed_items)\n",
    "    \n",
    "    # Preprocess dokumen mencurigakan dengan pemrosesan paralel\n",
    "    print(\"Melakukan preprocessing dokumen yang dicurigai...\")\n",
    "    suspicious_items = list(suspicious_docs.items())\n",
    "    suspicious_processed_items = Parallel(n_jobs=n_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(preprocess_document)(doc_item) for doc_item in tqdm(suspicious_items, desc=\"Processing suspicious docs\")\n",
    "    )\n",
    "    suspicious_processed_docs = dict(suspicious_processed_items)\n",
    "    \n",
    "    # Gabungkan semua dokumen terproses untuk analisis cross-similarity\n",
    "    all_processed_docs = {**source_processed_docs, **suspicious_processed_docs}\n",
    "    \n",
    "    # Hitung similaritas cosine\n",
    "    print(\"Menghitung similarity matrix...\")\n",
    "    docs_list = list(all_processed_docs.values())\n",
    "    filenames = list(all_processed_docs.keys())\n",
    "    \n",
    "    # Transformasi teks menjadi vektor TF-IDF (ini biasanya cepat dan tidak perlu paralelisasi)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs_list)\n",
    "    \n",
    "    # Fungsi untuk menghitung satu baris similarity matrix\n",
    "    def compute_similarity_row(i):\n",
    "        # Hanya menghitung segitiga atas dari matriks untuk efisiensi\n",
    "        row_similarities = cosine_similarity(tfidf_matrix[i:i+1], tfidf_matrix)[0]\n",
    "        return i, row_similarities\n",
    "    \n",
    "    # Hitung similarity secara paralel\n",
    "    similarity_rows = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "        delayed(compute_similarity_row)(i) for i in tqdm(range(len(docs_list)), desc=\"Computing similarities\")\n",
    "    )\n",
    "    \n",
    "    # Buat matriks similarity dari hasil\n",
    "    similarity_matrix = np.zeros((len(docs_list), len(docs_list)))\n",
    "    for i, row in similarity_rows:\n",
    "        similarity_matrix[i, :] = row\n",
    "    \n",
    "    # Konversi ke DataFrame untuk kemudahan visualisasi\n",
    "    similarity_matrix = pd.DataFrame(similarity_matrix, index=filenames, columns=filenames)\n",
    "    \n",
    "    # Visualisasikan similarity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, annot=False, cmap='YlGnBu')\n",
    "    plt.title('Similarity Matrix - Cosine Similarity between Documents')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Fungsi untuk menemukan pasangan dokumen dengan similarity di atas threshold\n",
    "    def find_high_similarity(i, threshold):\n",
    "        potential_pairs = []\n",
    "        source_indices = [idx for idx, name in enumerate(filenames) if name in source_processed_docs]\n",
    "        suspicious_indices = [idx for idx, name in enumerate(filenames) if name in suspicious_processed_docs]\n",
    "        \n",
    "        # Hanya bandingkan dokumen sumber dengan dokumen yang dicurigai\n",
    "        if i in source_indices:\n",
    "            for j in suspicious_indices:\n",
    "                sim_score = similarity_matrix.iloc[i, j]\n",
    "                if sim_score > threshold:\n",
    "                    doc1 = similarity_matrix.index[i]\n",
    "                    doc2 = similarity_matrix.columns[j]\n",
    "                    potential_pairs.append((doc1, doc2, sim_score))\n",
    "        return potential_pairs\n",
    "    \n",
    "    # Identifikasi potensi plagiarisme secara paralel (nilai similarity > threshold)\n",
    "    threshold = 0.7\n",
    "    print(f\"Mengidentifikasi potensi plagiarisme (threshold = {threshold})...\")\n",
    "    \n",
    "    # Hanya proses dokumen sumber (untuk mencegah duplikasi hasil)\n",
    "    source_indices = [idx for idx, name in enumerate(filenames) if name in source_processed_docs]\n",
    "    \n",
    "    all_pairs = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "        delayed(find_high_similarity)(i, threshold) \n",
    "        for i in tqdm(source_indices, desc=\"Finding similar pairs\")\n",
    "    )\n",
    "    \n",
    "    # Gabungkan semua hasil\n",
    "    potential_plagiarism = [pair for sublist in all_pairs for pair in sublist]\n",
    "    \n",
    "    # Tampilkan hasil\n",
    "    if potential_plagiarism:\n",
    "        print(\"Potential plagiarism detected:\")\n",
    "        for doc1, doc2, score in sorted(potential_plagiarism, key=lambda x: x[2], reverse=True):\n",
    "            print(f\"Source: '{doc1}' - Suspicious: '{doc2}' - Similarity: {score:.2f}\")\n",
    "    else:\n",
    "        print(f\"No potential plagiarism detected with threshold {threshold}.\")\n",
    "else:\n",
    "    print(\"No documents available for analysis. Please check the source and suspicious document directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd9ccf",
   "metadata": {},
   "source": [
    "## Deteksi Plagiarisme dengan N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a8757c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menghitung similaritas n-gram (n=3) antara dokumen sumber dan dokumen yang dicurigai...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing n-gram similarities:   1%|          | 107/10000 [00:07<11:40, 14.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source_name, source_content \u001b[38;5;129;01min\u001b[39;00m source_docs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m susp_name, susp_content \u001b[38;5;129;01min\u001b[39;00m suspicious_docs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# Hitung similarity\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m         sim_score \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_n_gram_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43msource_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43msusp_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_value\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m         n_gram_similarities[(source_name, susp_name)] \u001b[38;5;241m=\u001b[39m sim_score\n\u001b[1;32m     24\u001b[0m         comparison_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m, in \u001b[0;36mPlagiarismDetector.compute_n_gram_similarity\u001b[0;34m(self, doc1, doc2, n)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_n_gram_similarity\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc1, doc2, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Hitung similaritas berdasarkan n-gram\"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     ngrams1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_n_grams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     41\u001b[0m     ngrams2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_n_grams(doc2, n))\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Jaccard similarity\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m, in \u001b[0;36mPlagiarismDetector.extract_n_grams\u001b[0;34m(self, text, n)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_n_grams\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Ekstraksi n-gram dari teks\"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     n_grams \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m-\u001b[39m n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/nltk/tokenize/destructive.py:160\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    157\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPUNCTUATION:\n\u001b[0;32m--> 160\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Handles parentheses.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m regexp, substitution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPARENS_BRACKETS\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Menerapkan metode n-gram untuk deteksi plagiarisme\n",
    "if source_docs and suspicious_docs:\n",
    "    # Parameter n untuk n-gram\n",
    "    n_value = 3\n",
    "    \n",
    "    # Hitung similaritas n-gram antara dokumen sumber dan yang dicurigai\n",
    "    n_gram_similarities = {}\n",
    "    \n",
    "    print(f\"Menghitung similaritas n-gram (n={n_value}) antara dokumen sumber dan dokumen yang dicurigai...\")\n",
    "    \n",
    "    total_comparisons = len(source_docs) * len(suspicious_docs)\n",
    "    comparison_counter = 0\n",
    "    with tqdm(total=total_comparisons, desc=\"Computing n-gram similarities\") as pbar:\n",
    "        for source_name, source_content in source_docs.items():\n",
    "            for susp_name, susp_content in suspicious_docs.items():\n",
    "                # Hitung similarity\n",
    "                sim_score = detector.compute_n_gram_similarity(\n",
    "                    source_content, \n",
    "                    susp_content, \n",
    "                    n=n_value\n",
    "                )\n",
    "                \n",
    "                n_gram_similarities[(source_name, susp_name)] = sim_score\n",
    "                comparison_counter += 1\n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Tampilkan hasil n-gram similarity\n",
    "    print(f\"N-gram Similarity Results (n={n_value}):\")\n",
    "    for (doc1, doc2), score in sorted(n_gram_similarities.items(), key=lambda x: x[1], reverse=True)[:20]:  # Only show top 20\n",
    "        print(f\"Source: '{doc1}' - Suspicious: '{doc2}' - Similarity: {score:.4f}\")\n",
    "    \n",
    "    # Buat dataframe untuk visualisasi\n",
    "    results_data = []\n",
    "    for (doc1, doc2), ngram_score in n_gram_similarities.items():\n",
    "        cosine_score = similarity_matrix.loc[doc1, doc2]\n",
    "        results_data.append({\n",
    "            'Source': doc1,\n",
    "            'Suspicious': doc2,\n",
    "            'N-gram Similarity': ngram_score,\n",
    "            'Cosine Similarity': cosine_score\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Visualisasikan perbandingan antara metode n-gram dan cosine similarity\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(results_df['N-gram Similarity'], results_df['Cosine Similarity'], \n",
    "                alpha=0.6, edgecolors='w', s=100)\n",
    "    \n",
    "    # Tambahkan garis diagonal untuk referensi\n",
    "    max_val = max(results_df['N-gram Similarity'].max(), results_df['Cosine Similarity'].max())\n",
    "    plt.plot([0, max_val], [0, max_val], 'r--', alpha=0.5)\n",
    "    \n",
    "    plt.title('Comparison of N-gram and Cosine Similarity Methods')\n",
    "    plt.xlabel('N-gram Similarity')\n",
    "    plt.ylabel('Cosine Similarity')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualisasikan pasangan dokumen dengan similarity tertinggi\n",
    "    top_results = results_df.sort_values('Cosine Similarity', ascending=False).head(10)\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    x = range(len(top_results))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x, top_results['N-gram Similarity'], width, label='N-gram Similarity')\n",
    "    plt.bar([i + width for i in x], top_results['Cosine Similarity'], width, label='Cosine Similarity')\n",
    "    \n",
    "    plt.xlabel('Document Pairs')\n",
    "    plt.ylabel('Similarity Score')\n",
    "    plt.title('Top 10 Most Similar Document Pairs')\n",
    "    plt.xticks([i + width/2 for i in x], [f\"{s[:10]}...\\n{t[:10]}...\" for s, t in zip(top_results['Source'], top_results['Suspicious'])], \n",
    "              rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Need both source and suspicious documents for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0929326a",
   "metadata": {},
   "source": [
    "## Analisis Detil Kesamaan Text\n",
    "\n",
    "Mengidentifikasi bagian spesifik dari teks yang memiliki kesamaan tinggi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f959df2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'potential_plagiarism' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m similar_passages\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Demonstrasi analisis detil jika ada dokumen yang tersedia\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(documents) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mpotential_plagiarism\u001b[49m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Ambil pasangan dokumen dengan similaritas tertinggi\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     doc1_name, doc2_name, _ \u001b[38;5;241m=\u001b[39m potential_plagiarism[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDetailed similarity analysis between \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc1_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc2_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'potential_plagiarism' is not defined"
     ]
    }
   ],
   "source": [
    "def get_similar_passages(doc1, doc2, window_size=50, overlap=25, threshold=0.7):\n",
    "    \"\"\"Identify specific similar passages between two documents\"\"\"\n",
    "    # Split documents into sentences\n",
    "    sentences1 = sent_tokenize(doc1)\n",
    "    sentences2 = sent_tokenize(doc2)\n",
    "    \n",
    "    # Create windows of text (chunks)\n",
    "    def create_windows(sentences, window_size, overlap):\n",
    "        windows = []\n",
    "        flat_text = ' '.join(sentences)\n",
    "        tokens = word_tokenize(flat_text)\n",
    "        \n",
    "        for i in range(0, len(tokens), window_size - overlap):\n",
    "            if i + window_size <= len(tokens):\n",
    "                window = ' '.join(tokens[i:i+window_size])\n",
    "                windows.append(window)\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    windows1 = create_windows(sentences1, window_size, overlap)\n",
    "    windows2 = create_windows(sentences2, window_size, overlap)\n",
    "    \n",
    "    # Compute similarity between all pairs of windows\n",
    "    similar_passages = []\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    if not windows1 or not windows2:\n",
    "        return []\n",
    "    \n",
    "    # Combine all windows for vectorization\n",
    "    all_windows = windows1 + windows2\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_windows)\n",
    "    \n",
    "    # Split the matrix back\n",
    "    windows1_vectors = tfidf_matrix[:len(windows1)]\n",
    "    windows2_vectors = tfidf_matrix[len(windows1):]\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = cosine_similarity(windows1_vectors, windows2_vectors)\n",
    "    \n",
    "    # Find similar passages\n",
    "    for i in range(similarities.shape[0]):\n",
    "        for j in range(similarities.shape[1]):\n",
    "            if similarities[i, j] > threshold:\n",
    "                similar_passages.append({\n",
    "                    'doc1_passage': windows1[i],\n",
    "                    'doc2_passage': windows2[j],\n",
    "                    'similarity': similarities[i, j]\n",
    "                })\n",
    "    \n",
    "    # Sort by similarity score\n",
    "    similar_passages.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    return similar_passages\n",
    "\n",
    "# Demonstrasi analisis detil jika ada dokumen yang tersedia\n",
    "if documents and len(documents) > 1 and potential_plagiarism:\n",
    "    # Ambil pasangan dokumen dengan similaritas tertinggi\n",
    "    doc1_name, doc2_name, _ = potential_plagiarism[0]\n",
    "    \n",
    "    print(f\"\\nDetailed similarity analysis between '{doc1_name}' and '{doc2_name}':\\n\")\n",
    "    \n",
    "    # Dapatkan bagian teks yang mirip\n",
    "    similar_parts = get_similar_passages(\n",
    "        documents[doc1_name],\n",
    "        documents[doc2_name],\n",
    "        window_size=30,\n",
    "        overlap=15,\n",
    "        threshold=0.6\n",
    "    )\n",
    "    \n",
    "    # Tampilkan 3 bagian paling mirip\n",
    "    for i, match in enumerate(similar_parts[:3], 1):\n",
    "        print(f\"Match {i}: Similarity = {match['similarity']:.4f}\")\n",
    "        print(f\"Document 1: \\n{match['doc1_passage'][:100]}...\")\n",
    "        print(f\"Document 2: \\n{match['doc2_passage'][:100]}...\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"Insufficient documents or no potential plagiarism detected for detailed analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3cbf6",
   "metadata": {},
   "source": [
    "## Visualisasi Hasil Deteksi Plagiarisme\n",
    "\n",
    "Visualisasi tambahan untuk membantu analisis plagiarisme yang terdeteksi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi lebih lanjut untuk hasil deteksi plagiarisme\n",
    "if potential_plagiarism:\n",
    "    # Convert potential plagiarism list to DataFrame for easier analysis\n",
    "    plagiarism_df = pd.DataFrame(potential_plagiarism, columns=['source_doc', 'suspicious_doc', 'similarity_score'])\n",
    "    \n",
    "    # Sort by similarity score\n",
    "    plagiarism_df = plagiarism_df.sort_values('similarity_score', ascending=False)\n",
    "    \n",
    "    # Visualize top matches\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create a colorbar for similarity scores\n",
    "    cmap = plt.cm.YlOrRd\n",
    "    norm = plt.Normalize(plagiarism_df['similarity_score'].min(), plagiarism_df['similarity_score'].max())\n",
    "    \n",
    "    # Plot top 15 matches or all if less than 15\n",
    "    top_n = min(15, len(plagiarism_df))\n",
    "    bars = plt.barh(range(top_n), \n",
    "              plagiarism_df['similarity_score'].head(top_n), \n",
    "              color=cmap(norm(plagiarism_df['similarity_score'].head(top_n))))\n",
    "    \n",
    "    # Add color bar\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm)\n",
    "    cbar.set_label('Similarity Score')\n",
    "    \n",
    "    # Formatting\n",
    "    plt.yticks(range(top_n), \n",
    "              [f\"{s[:15]}... - {t[:15]}...\" for s, t in \n",
    "               zip(plagiarism_df['source_doc'].head(top_n), \n",
    "                   plagiarism_df['suspicious_doc'].head(top_n))])\n",
    "    plt.xlabel('Similarity Score')\n",
    "    plt.title('Top Document Pairs with Highest Similarity')\n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add similarity values as text\n",
    "    for i, bar in enumerate(bars):\n",
    "        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{plagiarism_df[\"similarity_score\"].iloc[i]:.3f}', \n",
    "                va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a heatmap for document similarities\n",
    "    # Create a matrix of source vs suspicious documents\n",
    "    source_docs_list = plagiarism_df['source_doc'].unique()\n",
    "    suspicious_docs_list = plagiarism_df['suspicious_doc'].unique()\n",
    "    \n",
    "    # Use only top 10 documents from each category for better visualization\n",
    "    source_docs_list = source_docs_list[:min(10, len(source_docs_list))]\n",
    "    suspicious_docs_list = suspicious_docs_list[:min(10, len(suspicious_docs_list))]\n",
    "    \n",
    "    # Create an empty heatmap matrix\n",
    "    heatmap_matrix = np.zeros((len(source_docs_list), len(suspicious_docs_list)))\n",
    "    \n",
    "    # Fill the matrix with similarity scores\n",
    "    for i, source_doc in enumerate(source_docs_list):\n",
    "        for j, suspicious_doc in enumerate(suspicious_docs_list):\n",
    "            # Find the entry in the dataframe\n",
    "            entry = plagiarism_df[(plagiarism_df['source_doc'] == source_doc) & \n",
    "                                 (plagiarism_df['suspicious_doc'] == suspicious_doc)]\n",
    "            \n",
    "            if not entry.empty:\n",
    "                heatmap_matrix[i, j] = entry['similarity_score'].iloc[0]\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.heatmap(heatmap_matrix, annot=True, fmt=\".2f\", cmap=\"YlOrRd\",\n",
    "                   xticklabels=[doc[:15] + \"...\" for doc in suspicious_docs_list],\n",
    "                   yticklabels=[doc[:15] + \"...\" for doc in source_docs_list])\n",
    "    \n",
    "    # Rotate x-axis labels for readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Suspicious Documents')\n",
    "    plt.ylabel('Source Documents')\n",
    "    plt.title('Heatmap of Document Similarity Scores')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No plagiarism data available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a9831",
   "metadata": {},
   "source": [
    "## Kesimpulan dan Ringkasan\n",
    "\n",
    "Dalam sistem deteksi plagiarisme ini, kita mengimplementasikan beberapa metode untuk mendeteksi kesamaan antara dokumen sumber dan dokumen yang dicurigai:\n",
    "\n",
    "1. **Pemrosesan Paralel** - Memanfaatkan paralelisme untuk mempercepat preprocessing dan perhitungan similarity pada dataset besar.\n",
    "\n",
    "2. **TF-IDF dan Cosine Similarity** - Metode dasar untuk mengukur kesamaan semantik antar dokumen.\n",
    "\n",
    "3. **N-gram Analysis** - Analisis berbasis n-gram untuk mendeteksi kesamaan struktur kalimat dan frasa.\n",
    "\n",
    "4. **Text Passage Analysis** - Analisis mendalam untuk mengidentifikasi bagian teks spesifik yang memiliki kemiripan tinggi.\n",
    "\n",
    "### Hasil Utama\n",
    "\n",
    "Sistem ini secara efektif dapat mengidentifikasi dokumen-dokumen yang memiliki kemiripan tinggi, dengan fokus pada perbandingan antara dokumen sumber dan dokumen yang dicurigai. Visualisasi membantu pengguna memahami pola dan tingkat kemiripan antar dokumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99de700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi performa sistem dan catatan untuk perbaikan di masa depan\n",
    "\n",
    "# 1. Hitung statistik dasar dari hasil deteksi\n",
    "if potential_plagiarism:\n",
    "    # Konversi ke DataFrame untuk analisis lebih lanjut\n",
    "    plag_df = pd.DataFrame(potential_plagiarism, columns=['doc1', 'doc2', 'similarity'])\n",
    "    \n",
    "    print(\"\\nRingkasan Statistik Similarity:\")\n",
    "    print(f\"Rata-rata similarity: {plag_df['similarity'].mean():.4f}\")\n",
    "    print(f\"Similarity tertinggi: {plag_df['similarity'].max():.4f}\")\n",
    "    print(f\"Similarity terendah: {plag_df['similarity'].min():.4f}\")\n",
    "    print(f\"Jumlah pasangan dokumen yang terdeteksi: {len(plag_df)}\")\n",
    "    \n",
    "    # Hitung distribusi nilai similarity\n",
    "    print(\"\\nDistribusi nilai similarity (berdasarkan rentang):\")\n",
    "    bins = [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "    labels = ['0.70-0.75', '0.75-0.80', '0.80-0.85', '0.85-0.90', '0.90-0.95', '0.95-1.00']\n",
    "    plag_df['similarity_range'] = pd.cut(plag_df['similarity'], bins=bins, labels=labels)\n",
    "    range_counts = plag_df['similarity_range'].value_counts().sort_index()\n",
    "    \n",
    "    for range_name, count in range_counts.items():\n",
    "        print(f\"Range {range_name}: {count} pasangan dokumen\")\n",
    "    \n",
    "    # Visualisasi distribusi similarity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(plag_df['similarity'], bins=20, kde=True)\n",
    "    plt.axvline(x=0.8, color='r', linestyle='--', alpha=0.7, label='Threshold 0.8')\n",
    "    plt.axvline(x=0.9, color='g', linestyle='--', alpha=0.7, label='Threshold 0.9')\n",
    "    plt.title('Distribusi Nilai Similarity')\n",
    "    plt.xlabel('Similarity Score')\n",
    "    plt.ylabel('Frekuensi')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Catatan untuk perbaikan di masa depan\n",
    "print(\"\\nCatatan untuk Perbaikan di Masa Depan:\")\n",
    "print(\"1. Implementasi metode deteksi berbasis struktur kalimat (syntactic analysis)\")\n",
    "print(\"2. Tambahkan dukungan untuk deteksi plagiarisme dalam banyak bahasa\")\n",
    "print(\"3. Tambahkan deteksi parafrase menggunakan model berbasis semantik yang lebih canggih\")\n",
    "print(\"4. Tingkatkan efisiensi pemrosesan untuk dataset yang lebih besar\")\n",
    "print(\"5. Integrasi dengan API eksternal untuk deteksi plagiarisme online\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e4f6a",
   "metadata": {},
   "source": [
    "## Pembuatan Model Klasifikasi untuk Deteksi Plagiarisme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3958f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Random Forest Classifier dengan Data Korpus Asli\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def train_random_forest_classifier():\n",
    "    \"\"\"Train a Random Forest classifier using actual corpus data\"\"\"\n",
    "    \n",
    "    print(\"Training Random Forest Classifier with Actual Corpus Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not (source_docs and suspicious_docs):\n",
    "        print(\"No source or suspicious documents available for training\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare feature extraction\n",
    "    print(\"Extracting features from document pairs...\")\n",
    "    \n",
    "    # Create document pairs and labels\n",
    "    document_pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    # Extract features using the same approach as stacked model\n",
    "    feature_extractor = PlagiarismDetector()\n",
    "    \n",
    "    # Process all document pairs\n",
    "    total_pairs = len(source_docs) * len(suspicious_docs)\n",
    "    processed_pairs = 0\n",
    "    \n",
    "    with tqdm(total=total_pairs, desc=\"Processing document pairs\") as pbar:\n",
    "        for source_name, source_content in source_docs.items():\n",
    "            for susp_name, susp_content in suspicious_docs.items():\n",
    "                # Extract comprehensive features\n",
    "                features = extract_comprehensive_features(source_content, susp_content)\n",
    "                document_pairs.append(features)\n",
    "                \n",
    "                # Create labels based on TF-IDF similarity threshold\n",
    "                processed_source = preprocess_text(source_content)\n",
    "                processed_susp = preprocess_text(susp_content)\n",
    "                \n",
    "                try:\n",
    "                    vectorizer_temp = TfidfVectorizer()\n",
    "                    tfidf_matrix = vectorizer_temp.fit_transform([processed_source, processed_susp])\n",
    "                    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "                except:\n",
    "                    similarity = 0\n",
    "                \n",
    "                # Label based on similarity threshold (0.6 for balanced dataset)\n",
    "                labels.append(1 if similarity > 0.6 else 0)\n",
    "                \n",
    "                processed_pairs += 1\n",
    "                pbar.update(1)\n",
    "    \n",
    "    X = np.array(document_pairs)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    print(f\"Dataset prepared: {len(X)} pairs\")\n",
    "    print(f\"Positive samples (plagiarism): {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "    print(f\"Negative samples (no plagiarism): {len(labels)-sum(labels)} ({(len(labels)-sum(labels))/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    # Handle NaN values\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    print(\"\\nTraining Random Forest...\")\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "    test_accuracy = rf_classifier.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"\\nRandom Forest Results:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(rf_classifier, X_train, y_train, cv=5)\n",
    "    print(f\"Cross-validation accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Negatives: {cm[0,0]}, False Positives: {cm[0,1]}\")\n",
    "    print(f\"False Negatives: {cm[1,0]}, True Positives: {cm[1,1]}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_names = [\n",
    "        \"TF-IDF Similarity\", \"Jaccard Similarity\", \"3-gram Similarity\", \n",
    "        \"5-gram Similarity\", \"Length Ratio\", \"Sentence Structure\",\n",
    "        \"Character Similarity\", \"Word Overlap\", \"Unique Words Ratio\",\n",
    "        \"Average Word Length\", \"Punctuation Similarity\", \"Digit Similarity\"\n",
    "    ]\n",
    "    \n",
    "    importances = rf_classifier.feature_importances_\n",
    "    \n",
    "    # Create feature importance visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('Random Forest Feature Importance (Sorted)')\n",
    "    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "    \n",
    "    # Confusion matrix heatmap\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['No Plagiarism', 'Plagiarism'],\n",
    "                yticklabels=['No Plagiarism', 'Plagiarism'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare with baseline methods\n",
    "    print(\"\\nComparing with Baseline Methods:\")\n",
    "    \n",
    "    # Simple TF-IDF threshold baseline\n",
    "    baseline_predictions = []\n",
    "    for features in X_test:\n",
    "        tfidf_sim = features[0]  # First feature is TF-IDF similarity\n",
    "        baseline_predictions.append(1 if tfidf_sim > 0.6 else 0)\n",
    "    \n",
    "    baseline_accuracy = accuracy_score(y_test, baseline_predictions)\n",
    "    print(f\"TF-IDF Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "    \n",
    "    # N-gram threshold baseline\n",
    "    ngram_predictions = []\n",
    "    for features in X_test:\n",
    "        ngram_sim = features[2]  # Third feature is 3-gram similarity\n",
    "        ngram_predictions.append(1 if ngram_sim > 0.3 else 0)\n",
    "    \n",
    "    ngram_accuracy = accuracy_score(y_test, ngram_predictions)\n",
    "    print(f\"N-gram Baseline Accuracy: {ngram_accuracy:.4f}\")\n",
    "    \n",
    "    print(f\"Random Forest Improvement over TF-IDF: {(test_accuracy - baseline_accuracy)*100:.2f}%\")\n",
    "    print(f\"Random Forest Improvement over N-gram: {(test_accuracy - ngram_accuracy)*100:.2f}%\")\n",
    "    \n",
    "    return rf_classifier\n",
    "\n",
    "def extract_comprehensive_features(doc1, doc2):\n",
    "    \"\"\"Extract comprehensive features for Random Forest training\"\"\"\n",
    "    \n",
    "    # Preprocess documents\n",
    "    processed_doc1 = preprocess_text(doc1)\n",
    "    processed_doc2 = preprocess_text(doc2)\n",
    "    \n",
    "    # Feature 1: TF-IDF Cosine Similarity\n",
    "    try:\n",
    "        vectorizer_temp = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer_temp.fit_transform([processed_doc1, processed_doc2])\n",
    "        tfidf_similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "    except:\n",
    "        tfidf_similarity = 0\n",
    "    \n",
    "    # Feature 2: Jaccard Similarity\n",
    "    tokens1 = set(word_tokenize(processed_doc1))\n",
    "    tokens2 = set(word_tokenize(processed_doc2))\n",
    "    if tokens1 or tokens2:\n",
    "        jaccard_similarity = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
    "    else:\n",
    "        jaccard_similarity = 0\n",
    "    \n",
    "    # Feature 3-4: N-gram similarities\n",
    "    def get_ngrams(text, n):\n",
    "        tokens = word_tokenize(text)\n",
    "        return set([' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)])\n",
    "    \n",
    "    ngrams1_3 = get_ngrams(processed_doc1, 3)\n",
    "    ngrams2_3 = get_ngrams(processed_doc2, 3)\n",
    "    ngram3_similarity = len(ngrams1_3.intersection(ngrams2_3)) / len(ngrams1_3.union(ngrams2_3)) if (ngrams1_3 or ngrams2_3) else 0\n",
    "    \n",
    "    ngrams1_5 = get_ngrams(processed_doc1, 5)\n",
    "    ngrams2_5 = get_ngrams(processed_doc2, 5)\n",
    "    ngram5_similarity = len(ngrams1_5.intersection(ngrams2_5)) / len(ngrams1_5.union(ngrams2_5)) if (ngrams1_5 or ngrams2_5) else 0\n",
    "    \n",
    "    # Feature 5: Length ratio\n",
    "    len1, len2 = len(processed_doc1.split()), len(processed_doc2.split())\n",
    "    length_ratio = min(len1, len2) / max(len1, len2) if max(len1, len2) > 0 else 0\n",
    "    \n",
    "    # Feature 6: Sentence structure similarity\n",
    "    sent1 = sent_tokenize(doc1)\n",
    "    sent2 = sent_tokenize(doc2)\n",
    "    avg_sent_len1 = len(doc1.split()) / max(1, len(sent1))\n",
    "    avg_sent_len2 = len(doc2.split()) / max(1, len(sent2))\n",
    "    sent_similarity = min(avg_sent_len1, avg_sent_len2) / max(avg_sent_len1, avg_sent_len2) if max(avg_sent_len1, avg_sent_len2) > 0 else 0\n",
    "    \n",
    "    # Feature 7: Character-level similarity\n",
    "    char_similarity = difflib.SequenceMatcher(None, processed_doc1, processed_doc2).ratio()\n",
    "    \n",
    "    # Feature 8: Word overlap ratio\n",
    "    words1 = set(processed_doc1.split())\n",
    "    words2 = set(processed_doc2.split())\n",
    "    word_overlap = len(words1.intersection(words2)) / len(words1.union(words2)) if (words1 or words2) else 0\n",
    "    \n",
    "    # Feature 9: Unique words ratio\n",
    "    unique1 = len(words1 - words2)\n",
    "    unique2 = len(words2 - words1)\n",
    "    total_unique = unique1 + unique2\n",
    "    unique_ratio = total_unique / max(1, len(words1.union(words2)))\n",
    "    \n",
    "    # Feature 10: Average word length similarity\n",
    "    if words1 and words2:\n",
    "        avg_len1 = sum(len(word) for word in words1) / len(words1)\n",
    "        avg_len2 = sum(len(word) for word in words2) / len(words2)\n",
    "        word_len_similarity = min(avg_len1, avg_len2) / max(avg_len1, avg_len2)\n",
    "    else:\n",
    "        word_len_similarity = 0\n",
    "    \n",
    "    # Feature 11: Punctuation similarity\n",
    "    punct1 = re.findall(r'[^\\w\\s]', doc1)\n",
    "    punct2 = re.findall(r'[^\\w\\s]', doc2)\n",
    "    punct_similarity = len(set(punct1).intersection(set(punct2))) / max(1, len(set(punct1).union(set(punct2))))\n",
    "    \n",
    "    # Feature 12: Digit similarity\n",
    "    digits1 = re.findall(r'\\d+', doc1)\n",
    "    digits2 = re.findall(r'\\d+', doc2)\n",
    "    digit_similarity = len(set(digits1).intersection(set(digits2))) / max(1, len(set(digits1).union(set(digits2))))\n",
    "    \n",
    "    return [\n",
    "        tfidf_similarity, jaccard_similarity, ngram3_similarity, ngram5_similarity,\n",
    "        length_ratio, sent_similarity, char_similarity, word_overlap,\n",
    "        unique_ratio, word_len_similarity, punct_similarity, digit_similarity\n",
    "    ]\n",
    "\n",
    "# Train Random Forest classifier with actual data\n",
    "if source_docs and suspicious_docs:\n",
    "    print(\"Training Random Forest classifier with actual corpus data...\")\n",
    "    rf_model = train_random_forest_classifier()\n",
    "    \n",
    "    if rf_model:\n",
    "        print(\"\\nRandom Forest model training completed successfully!\")\n",
    "        \n",
    "        # Save the model\n",
    "        import joblib\n",
    "        rf_model_path = \"../models/random_forest_plagiarism_detector.joblib\"\n",
    "        os.makedirs(\"../models\", exist_ok=True)\n",
    "        joblib.dump(rf_model, rf_model_path)\n",
    "        print(f\"Random Forest model saved to: {rf_model_path}\")\n",
    "else:\n",
    "    print(\"No documents available for Random Forest training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3084f9a8",
   "metadata": {},
   "source": [
    "## Kesimpulan\n",
    "\n",
    "Notebook ini telah mendemonstrasikan beberapa teknik untuk mendeteksi plagiarisme:\n",
    "\n",
    "1. **Preprocessing Teks**: Tokenisasi, menghilangkan stopwords, dan stemming untuk bahasa Indonesia.\n",
    "2. **Ekstraksi Fitur**: TF-IDF untuk representasi dokumen.\n",
    "3. **Metode Perbandingan**: Cosine similarity dan N-gram similarity.\n",
    "4. **Analisis Detil**: Identifikasi bagian teks yang memiliki kesamaan tinggi.\n",
    "5. **Model Klasifikasi**: Simulasi penggunaan model machine learning untuk klasifikasi plagiarisme.\n",
    "\n",
    "Untuk pengembangan selanjutnya, beberapa hal yang bisa dilakukan:\n",
    "- Gunakan dataset yang lebih besar dan berlabel untuk evaluasi yang lebih baik.\n",
    "- Implementasikan algoritma fingerprinting seperti Winnowing atau Rabin-Karp.\n",
    "- Eksplorasi metode NLP lanjutan seperti word embeddings atau transformers untuk perbandingan semantik.\n",
    "- Integrasi deteksi plagiarisme lintas bahasa (cross-language plagiarism detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccadcd6",
   "metadata": {},
   "source": [
    "## Implementasi Model Hybrid untuk Deteksi Plagiarisme\n",
    "\n",
    "Model hybrid menggabungkan beberapa teknik deteksi plagiarisme untuk menghasilkan deteksi yang lebih akurat dan robust. Pendekatan ini menggabungkan kekuatan dari berbagai metode dan dapat mengatasi kelemahan dari masing-masing metode individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for logistic regression and XGBoost\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class StackedPlagiarismDetector:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize stacked model for plagiarism detection with base models and meta model\"\"\"\n",
    "        # Base models\n",
    "        self.logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        self.xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "        \n",
    "        # Meta model\n",
    "        self.meta_model = LogisticRegression(random_state=42)\n",
    "        \n",
    "        # Feature preprocessing\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Initialize vectorizer and other tools\n",
    "        self.vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        self.stopwords = set(stopwords.words('indonesian'))\n",
    "        factory = StemmerFactory()\n",
    "        self.stemmer = factory.create_stemmer()\n",
    "        \n",
    "        # Track if models have been trained\n",
    "        self.is_trained = False\n",
    "        \n",
    "        # Model performance metrics\n",
    "        self.training_metrics = {}\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Preprocess text for feature extraction\"\"\"\n",
    "        # Lowercase and remove special characters\n",
    "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        tokens = [word for word in tokens if word not in self.stopwords]\n",
    "        \n",
    "        # Apply stemming\n",
    "        tokens = [self.stemmer.stem(word) for word in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def extract_features(self, doc1, doc2):\n",
    "        \"\"\"Extract features from document pair for model training/prediction\"\"\"\n",
    "        # Preprocess documents\n",
    "        processed_doc1 = self.preprocess(doc1)\n",
    "        processed_doc2 = self.preprocess(doc2)\n",
    "        \n",
    "        # Feature 1: TF-IDF Cosine Similarity\n",
    "        try:\n",
    "            tfidf_matrix = self.vectorizer.fit_transform([processed_doc1, processed_doc2])\n",
    "            tfidf_similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        except:\n",
    "            tfidf_similarity = 0\n",
    "        \n",
    "        # Feature 2: Jaccard Similarity of word sets\n",
    "        tokens1 = set(word_tokenize(processed_doc1))\n",
    "        tokens2 = set(word_tokenize(processed_doc2))\n",
    "        if tokens1 or tokens2:  # Avoid division by zero\n",
    "            jaccard_similarity = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
    "        else:\n",
    "            jaccard_similarity = 0\n",
    "        \n",
    "        # Feature 3: 3-gram similarity\n",
    "        def get_ngrams(text, n):\n",
    "            tokens = word_tokenize(text)\n",
    "            return set([' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)])\n",
    "        \n",
    "        ngrams1 = get_ngrams(processed_doc1, 3)\n",
    "        ngrams2 = get_ngrams(processed_doc2, 3)\n",
    "        if ngrams1 or ngrams2:  # Avoid division by zero\n",
    "            ngram_similarity = len(ngrams1.intersection(ngrams2)) / len(ngrams1.union(ngrams2))\n",
    "        else:\n",
    "            ngram_similarity = 0\n",
    "        \n",
    "        # Feature 4: 5-gram similarity\n",
    "        ngrams1_5 = get_ngrams(processed_doc1, 5)\n",
    "        ngrams2_5 = get_ngrams(processed_doc2, 5)\n",
    "        if ngrams1_5 or ngrams2_5:\n",
    "            ngram5_similarity = len(ngrams1_5.intersection(ngrams2_5)) / len(ngrams1_5.union(ngrams2_5))\n",
    "        else:\n",
    "            ngram5_similarity = 0\n",
    "        \n",
    "        # Feature 5: Length ratio (shorter/longer)\n",
    "        len1, len2 = len(processed_doc1.split()), len(processed_doc2.split())\n",
    "        length_ratio = min(len1, len2) / max(len1, len2) if max(len1, len2) > 0 else 0\n",
    "        \n",
    "        # Feature 6: Common sentence structure\n",
    "        sent1 = sent_tokenize(doc1)\n",
    "        sent2 = sent_tokenize(doc2)\n",
    "        avg_sent_len1 = len(doc1.split()) / max(1, len(sent1))\n",
    "        avg_sent_len2 = len(doc2.split()) / max(1, len(sent2))\n",
    "        sent_len_similarity = min(avg_sent_len1, avg_sent_len2) / max(avg_sent_len1, avg_sent_len2) if max(avg_sent_len1, avg_sent_len2) > 0 else 0\n",
    "        \n",
    "        # Feature 7: Character-level similarity\n",
    "        char_similarity = difflib.SequenceMatcher(None, processed_doc1, processed_doc2).ratio()\n",
    "        \n",
    "        # Feature 8: Word overlap ratio\n",
    "        words1 = set(processed_doc1.split())\n",
    "        words2 = set(processed_doc2.split())\n",
    "        if words1 or words2:\n",
    "            word_overlap = len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "        else:\n",
    "            word_overlap = 0\n",
    "        \n",
    "        # Return feature vector\n",
    "        return [\n",
    "            tfidf_similarity,   \n",
    "            jaccard_similarity, \n",
    "            ngram_similarity,   \n",
    "            ngram5_similarity,  \n",
    "            length_ratio,       \n",
    "            sent_len_similarity,\n",
    "            char_similarity,\n",
    "            word_overlap\n",
    "        ]\n",
    "    \n",
    "    def prepare_training_data_from_corpus(self, source_docs, suspicious_docs, similarity_threshold=0.7):\n",
    "        \"\"\"Prepare training data from the actual corpus\"\"\"\n",
    "        print(\"Preparing training data from corpus...\")\n",
    "        \n",
    "        training_pairs = []\n",
    "        labels = []\n",
    "        \n",
    "        # Calculate similarity between all source and suspicious document pairs\n",
    "        total_pairs = len(source_docs) * len(suspicious_docs)\n",
    "        processed_pairs = 0\n",
    "        \n",
    "        with tqdm(total=total_pairs, desc=\"Processing document pairs\") as pbar:\n",
    "            for source_name, source_content in source_docs.items():\n",
    "                for susp_name, susp_content in suspicious_docs.items():\n",
    "                    # Calculate a quick similarity score to label the data\n",
    "                    processed_source = self.preprocess(source_content)\n",
    "                    processed_susp = self.preprocess(susp_content)\n",
    "                    \n",
    "                    # Use TF-IDF similarity as the labeling criterion\n",
    "                    try:\n",
    "                        tfidf_matrix = self.vectorizer.fit_transform([processed_source, processed_susp])\n",
    "                        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "                    except:\n",
    "                        similarity = 0\n",
    "                    \n",
    "                    # Add to training data\n",
    "                    training_pairs.append((source_content, susp_content))\n",
    "                    \n",
    "                    # Label based on similarity threshold\n",
    "                    labels.append(1 if similarity > similarity_threshold else 0)\n",
    "                    \n",
    "                    processed_pairs += 1\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        print(f\"Prepared {len(training_pairs)} document pairs\")\n",
    "        print(f\"Positive samples (plagiarism): {sum(labels)}\")\n",
    "        print(f\"Negative samples (no plagiarism): {len(labels) - sum(labels)}\")\n",
    "        \n",
    "        return training_pairs, labels\n",
    "    \n",
    "    def train(self, document_pairs, labels):\n",
    "        \"\"\"\n",
    "        Train the stacked model using the following steps:\n",
    "        1. Train base models (Logistic Regression and XGBoost) on the training data\n",
    "        2. Use base models to make predictions on training data\n",
    "        3. Train meta model on the predictions from base models\n",
    "        \"\"\"\n",
    "        print(\"Extracting features from document pairs...\")\n",
    "        \n",
    "        # Extract features from document pairs\n",
    "        X = []\n",
    "        for i, (doc1, doc2) in enumerate(tqdm(document_pairs, desc=\"Extracting features\")):\n",
    "            try:\n",
    "                features = self.extract_features(doc1, doc2)\n",
    "                X.append(features)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing pair {i}: {e}\")\n",
    "                # Add zero features if extraction fails\n",
    "                X.append([0] * 8)\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(labels)\n",
    "        \n",
    "        print(f\"Features shape: {X.shape}\")\n",
    "        print(f\"Labels shape: {y.shape}\")\n",
    "        \n",
    "        # Remove any NaN or infinite values\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "        \n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        # Scale features\n",
    "        print(\"Scaling features...\")\n",
    "        self.scaler.fit(X_train)\n",
    "        X_train_scaled = self.scaler.transform(X_train)\n",
    "        X_val_scaled = self.scaler.transform(X_val)\n",
    "        \n",
    "        # Train base models\n",
    "        print(\"Training base models...\")\n",
    "        \n",
    "        # Train Logistic Regression\n",
    "        print(\"Training Logistic Regression...\")\n",
    "        self.logistic_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Train XGBoost\n",
    "        print(\"Training XGBoost...\")\n",
    "        self.xgb_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Get base model predictions for meta-model training\n",
    "        print(\"Getting base model predictions for meta-model...\")\n",
    "        logistic_preds = self.logistic_model.predict_proba(X_val_scaled)[:, 1].reshape(-1, 1)\n",
    "        xgb_preds = self.xgb_model.predict_proba(X_val_scaled)[:, 1].reshape(-1, 1)\n",
    "        \n",
    "        # Combine predictions as features for meta-model\n",
    "        meta_features = np.hstack([logistic_preds, xgb_preds])\n",
    "        \n",
    "        # Train meta-model\n",
    "        print(\"Training meta model...\")\n",
    "        self.meta_model.fit(meta_features, y_val)\n",
    "        \n",
    "        # Calculate training accuracies\n",
    "        logistic_acc = accuracy_score(y_val, (logistic_preds >= 0.5).astype(int))\n",
    "        xgb_acc = accuracy_score(y_val, (xgb_preds >= 0.5).astype(int))\n",
    "        meta_acc = accuracy_score(y_val, self.meta_model.predict(meta_features))\n",
    "        \n",
    "        # Calculate cross-validation scores\n",
    "        print(\"Calculating cross-validation scores...\")\n",
    "        cv_scores_lr = cross_val_score(self.logistic_model, X_train_scaled, y_train, cv=5)\n",
    "        cv_scores_xgb = cross_val_score(self.xgb_model, X_train_scaled, y_train, cv=5)\n",
    "        \n",
    "        # Store training metrics\n",
    "        self.training_metrics = {\n",
    "            'logistic_accuracy': logistic_acc,\n",
    "            'xgboost_accuracy': xgb_acc,\n",
    "            'meta_accuracy': meta_acc,\n",
    "            'logistic_cv_mean': cv_scores_lr.mean(),\n",
    "            'logistic_cv_std': cv_scores_lr.std(),\n",
    "            'xgboost_cv_mean': cv_scores_xgb.mean(),\n",
    "            'xgboost_cv_std': cv_scores_xgb.std(),\n",
    "            'training_samples': len(X_train),\n",
    "            'validation_samples': len(X_val)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nTraining Results:\")\n",
    "        print(f\"Base model accuracies - Logistic: {logistic_acc:.4f}, XGBoost: {xgb_acc:.4f}\")\n",
    "        print(f\"Meta model accuracy: {meta_acc:.4f}\")\n",
    "        print(f\"Logistic CV: {cv_scores_lr.mean():.4f} ± {cv_scores_lr.std():.4f}\")\n",
    "        print(f\"XGBoost CV: {cv_scores_xgb.mean():.4f} ± {cv_scores_xgb.std():.4f}\")\n",
    "        \n",
    "        # Generate classification reports\n",
    "        logistic_pred_labels = (logistic_preds >= 0.5).astype(int)\n",
    "        xgb_pred_labels = (xgb_preds >= 0.5).astype(int)\n",
    "        meta_pred_labels = self.meta_model.predict(meta_features)\n",
    "        \n",
    "        print(\"\\nLogistic Regression Classification Report:\")\n",
    "        print(classification_report(y_val, logistic_pred_labels))\n",
    "        \n",
    "        print(\"\\nXGBoost Classification Report:\")\n",
    "        print(classification_report(y_val, xgb_pred_labels))\n",
    "        \n",
    "        print(\"\\nMeta Model Classification Report:\")\n",
    "        print(classification_report(y_val, meta_pred_labels))\n",
    "        \n",
    "        # Mark as trained\n",
    "        self.is_trained = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, doc1, doc2):\n",
    "        \"\"\"Predict plagiarism using the trained stacked model\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before prediction\")\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.extract_features(doc1, doc2)\n",
    "        features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "        features_scaled = self.scaler.transform([features])\n",
    "        \n",
    "        # Get base model predictions\n",
    "        logistic_pred = self.logistic_model.predict_proba(features_scaled)[:, 1].reshape(-1, 1)\n",
    "        xgb_pred = self.xgb_model.predict_proba(features_scaled)[:, 1].reshape(-1, 1)\n",
    "        \n",
    "        # Combine for meta model\n",
    "        meta_features = np.hstack([logistic_pred, xgb_pred])\n",
    "        \n",
    "        # Meta model prediction\n",
    "        final_pred = self.meta_model.predict(meta_features)[0]\n",
    "        confidence = self.meta_model.predict_proba(meta_features)[0][final_pred]\n",
    "        \n",
    "        # Return prediction results with model details\n",
    "        return {\n",
    "            'prediction': final_pred,  # 0: No plagiarism, 1: Plagiarism\n",
    "            'confidence': confidence,\n",
    "            'base_model_outputs': {\n",
    "                'logistic_regression': float(logistic_pred[0][0]),\n",
    "                'xgboost': float(xgb_pred[0][0])\n",
    "            },\n",
    "            'original_features': features\n",
    "        }\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model to disk\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before saving\")\n",
    "        \n",
    "        model_data = {\n",
    "            'logistic_model': self.logistic_model,\n",
    "            'xgb_model': self.xgb_model,\n",
    "            'meta_model': self.meta_model,\n",
    "            'scaler': self.scaler,\n",
    "            'vectorizer': self.vectorizer,\n",
    "            'training_metrics': self.training_metrics,\n",
    "            'is_trained': self.is_trained\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained model from disk\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        self.logistic_model = model_data['logistic_model']\n",
    "        self.xgb_model = model_data['xgb_model']\n",
    "        self.meta_model = model_data['meta_model']\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.vectorizer = model_data['vectorizer']\n",
    "        self.training_metrics = model_data['training_metrics']\n",
    "        self.is_trained = model_data['is_trained']\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "# Now train the model with actual data\n",
    "if source_docs and suspicious_docs:\n",
    "    print(\"Training stacked model with actual corpus data...\")\n",
    "    \n",
    "    # Create stacked model\n",
    "    stacked_detector = StackedPlagiarismDetector()\n",
    "    \n",
    "    # Prepare training data from corpus\n",
    "    training_pairs, training_labels = stacked_detector.prepare_training_data_from_corpus(\n",
    "        source_docs, suspicious_docs, similarity_threshold=0.6\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    if len(training_pairs) > 0:\n",
    "        stacked_detector.train(training_pairs, training_labels)\n",
    "        \n",
    "        # Save the trained model\n",
    "        model_save_path = \"../plagiarism_stacked_model.pkl\"\n",
    "        stacked_detector.save_model(model_save_path)\n",
    "        \n",
    "        print(f\"\\nModel training completed and saved to {model_save_path}\")\n",
    "        print(\"\\nTraining Metrics:\")\n",
    "        for metric, value in stacked_detector.training_metrics.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No training data available\")\n",
    "else:\n",
    "    print(\"No source or suspicious documents available for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe876753",
   "metadata": {},
   "source": [
    "## Implementasi Metamodel untuk Deteksi Plagiarisme\n",
    "\n",
    "Metamodel adalah pendekatan yang menggabungkan output dari beberapa model individu dan menggunakan model lain (meta-learner) untuk membuat prediksi final. Dalam konteks deteksi plagiarisme, metamodel menggunakan hasil berbagai algoritma deteksi plagiarisme sebagai input untuk membuat keputusan final tentang tingkat plagiarisme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e37ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell has been replaced by the StackedPlagiarismDetector class above\n",
    "# The new implementation provides a more sophisticated stacked model approach\n",
    "# that aligns with the requested flow: training base models (Logistic Regression and XGBoost)\n",
    "# on the training set, then using their outputs to train a meta model for final prediction.\n",
    "\n",
    "# For reference, see the StackedPlagiarismDetector class implementation above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f5e20",
   "metadata": {},
   "source": [
    "## Perbandingan dan Evaluasi Model\n",
    "\n",
    "Bagian ini membandingkan kinerja model hybrid dan metamodel dengan model dasar (baseline) untuk mengevaluasi efektivitasnya dalam deteksi plagiarisme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e163d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trained_models():\n",
    "    \"\"\"Evaluate the trained models on test data and compare performance\"\"\"\n",
    "    print(\"Evaluating Trained Models on Corpus Data\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if we have trained models\n",
    "    if 'stacked_detector' not in globals() or not stacked_detector.is_trained:\n",
    "        print(\"No trained model found. Please run the training cell first.\")\n",
    "        return\n",
    "    \n",
    "    # Test on some document pairs from our corpus\n",
    "    test_results = []\n",
    "    \n",
    "    # Take first 5 source and suspicious documents for testing\n",
    "    source_items = list(source_docs.items())[:5]\n",
    "    suspicious_items = list(suspicious_docs.items())[:5]\n",
    "    \n",
    "    print(\"Testing on document pairs...\")\n",
    "    \n",
    "    for i, (source_name, source_content) in enumerate(source_items):\n",
    "        for j, (susp_name, susp_content) in enumerate(suspicious_items):\n",
    "            # Get predictions from stacked model\n",
    "            result = stacked_detector.predict(source_content, susp_content)\n",
    "            \n",
    "            # Get baseline similarity using TF-IDF cosine similarity\n",
    "            processed_source = stacked_detector.preprocess(source_content)\n",
    "            processed_susp = stacked_detector.preprocess(susp_content)\n",
    "            \n",
    "            try:\n",
    "                tfidf_matrix = TfidfVectorizer().fit_transform([processed_source, processed_susp])\n",
    "                baseline_similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            except:\n",
    "                baseline_similarity = 0\n",
    "            \n",
    "            # Get n-gram similarity\n",
    "            ngram_similarity = detector.compute_n_gram_similarity(source_content, susp_content, n=3)\n",
    "            \n",
    "            test_results.append({\n",
    "                'Source': source_name[:20] + '...',\n",
    "                'Suspicious': susp_name[:20] + '...',\n",
    "                'Baseline_TF-IDF': baseline_similarity,\n",
    "                'N-gram_Similarity': ngram_similarity,\n",
    "                'Logistic_Score': result['base_model_outputs']['logistic_regression'],\n",
    "                'XGBoost_Score': result['base_model_outputs']['xgboost'],\n",
    "                'Stacked_Prediction': result['prediction'],\n",
    "                'Stacked_Confidence': result['confidence']\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame for better visualization\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nModel Comparison Results (First 10 pairs):\")\n",
    "    print(results_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Calculate some statistics\n",
    "    print(\"\\nStatistics:\")\n",
    "    print(f\"Average Baseline TF-IDF: {results_df['Baseline_TF-IDF'].mean():.4f}\")\n",
    "    print(f\"Average N-gram Similarity: {results_df['N-gram_Similarity'].mean():.4f}\")\n",
    "    print(f\"Average Logistic Score: {results_df['Logistic_Score'].mean():.4f}\")\n",
    "    print(f\"Average XGBoost Score: {results_df['XGBoost_Score'].mean():.4f}\")\n",
    "    print(f\"Plagiarism Detection Rate: {results_df['Stacked_Prediction'].mean():.4f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Distribution of similarity scores\n",
    "    axes[0, 0].hist(results_df['Baseline_TF-IDF'], alpha=0.5, label='Baseline TF-IDF', bins=20)\n",
    "    axes[0, 0].hist(results_df['Logistic_Score'], alpha=0.5, label='Logistic Regression', bins=20)\n",
    "    axes[0, 0].hist(results_df['XGBoost_Score'], alpha=0.5, label='XGBoost', bins=20)\n",
    "    axes[0, 0].set_xlabel('Similarity Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Distribution of Similarity Scores')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Correlation between methods\n",
    "    axes[0, 1].scatter(results_df['Baseline_TF-IDF'], results_df['Logistic_Score'], alpha=0.6, label='Logistic vs Baseline')\n",
    "    axes[0, 1].scatter(results_df['Baseline_TF-IDF'], results_df['XGBoost_Score'], alpha=0.6, label='XGBoost vs Baseline')\n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'r--', alpha=0.5)\n",
    "    axes[0, 1].set_xlabel('Baseline TF-IDF Score')\n",
    "    axes[0, 1].set_ylabel('Model Score')\n",
    "    axes[0, 1].set_title('Model Scores vs Baseline')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Plagiarism classification distribution\n",
    "    pred_counts = results_df['Stacked_Prediction'].value_counts()\n",
    "    labels = ['No Plagiarism', 'Plagiarism']\n",
    "    axes[1, 0].pie(pred_counts.values, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    axes[1, 0].set_title('Stacked Model Predictions Distribution')\n",
    "    \n",
    "    # Plot 4: Confidence distribution for predictions\n",
    "    plagiarism_conf = results_df[results_df['Stacked_Prediction'] == 1]['Stacked_Confidence']\n",
    "    no_plagiarism_conf = results_df[results_df['Stacked_Prediction'] == 0]['Stacked_Confidence']\n",
    "    \n",
    "    if len(plagiarism_conf) > 0:\n",
    "        axes[1, 1].hist(plagiarism_conf, alpha=0.7, label='Plagiarism', bins=10)\n",
    "    if len(no_plagiarism_conf) > 0:\n",
    "        axes[1, 1].hist(no_plagiarism_conf, alpha=0.7, label='No Plagiarism', bins=10)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Confidence Score')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Prediction Confidence Distribution')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show feature importance from trained models\n",
    "    if hasattr(stacked_detector.logistic_model, 'coef_'):\n",
    "        feature_names = [\n",
    "            \"TF-IDF Similarity\", \n",
    "            \"Jaccard Similarity\", \n",
    "            \"3-gram Similarity\", \n",
    "            \"5-gram Similarity\", \n",
    "            \"Length Ratio\", \n",
    "            \"Sentence Structure\",\n",
    "            \"Character Similarity\",\n",
    "            \"Word Overlap\"\n",
    "        ]\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # Logistic Regression coefficients\n",
    "        coef = stacked_detector.logistic_model.coef_[0]\n",
    "        axes[0].bar(range(len(feature_names)), coef)\n",
    "        axes[0].set_xticks(range(len(feature_names)))\n",
    "        axes[0].set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "        axes[0].set_ylabel('Coefficient Value')\n",
    "        axes[0].set_title('Logistic Regression Feature Importance')\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        \n",
    "        # XGBoost feature importance\n",
    "        if hasattr(stacked_detector.xgb_model, 'feature_importances_'):\n",
    "            importances = stacked_detector.xgb_model.feature_importances_\n",
    "            axes[1].bar(range(len(feature_names)), importances)\n",
    "            axes[1].set_xticks(range(len(feature_names)))\n",
    "            axes[1].set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "            axes[1].set_ylabel('Importance')\n",
    "            axes[1].set_title('XGBoost Feature Importance')\n",
    "            axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        # Meta model coefficients\n",
    "        if hasattr(stacked_detector.meta_model, 'coef_'):\n",
    "            meta_coef = stacked_detector.meta_model.coef_[0]\n",
    "            axes[2].bar(['Logistic Regression', 'XGBoost'], meta_coef)\n",
    "            axes[2].set_ylabel('Coefficient Value')\n",
    "            axes[2].set_title('Meta Model: Base Model Importance')\n",
    "            axes[2].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Display training metrics\n",
    "    print(\"\\nTraining Metrics Summary:\")\n",
    "    print(\"=\" * 30)\n",
    "    for metric, value in stacked_detector.training_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run the evaluation\n",
    "if 'stacked_detector' in globals():\n",
    "    evaluation_results = evaluate_trained_models()\n",
    "else:\n",
    "    print(\"Please run the training cell first to create the stacked_detector.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86ab87c",
   "metadata": {},
   "source": [
    "## Kesimpulan dan Pengembangan Selanjutnya\n",
    "\n",
    "Berdasarkan implementasi dan eksperimen yang telah dilakukan, model stacked untuk deteksi plagiarisme menunjukkan performa yang menjanjikan. Pendekatan stacked modeling ini menggabungkan kekuatan dari berbagai algoritma machine learning untuk menghasilkan sistem deteksi plagiarisme yang lebih robust dan akurat.\n",
    "\n",
    "### Kesimpulan:\n",
    "\n",
    "1. **Stacked Model Approach**: Implementasi model stacked dengan Logistic Regression dan XGBoost sebagai base model, serta meta-model untuk prediksi final menunjukkan peningkatan performa dibandingkan dengan model tunggal. Pendekatan ini mampu mengatasi kelemahan dari metode individu dan memberikan hasil yang lebih konsisten.\n",
    "\n",
    "2. **Base Models (Logistic Regression dan XGBoost)**: Kedua model dasar ini berhasil mempelajari pola dari fitur-fitur yang diekstrak dari pasangan dokumen. Logistic Regression cenderung memberikan hasil yang lebih stabil, sementara XGBoost mampu menangkap pola non-linear yang lebih kompleks.\n",
    "\n",
    "3. **Meta Model**: Meta model berhasil mengintegrasikan prediksi dari model dasar untuk menghasilkan keputusan final yang lebih akurat. Dengan mempelajari bagaimana menggabungkan output dari model dasar, meta model dapat memberikan hasil yang lebih baik daripada masing-masing model dasar secara individual.\n",
    "\n",
    "4. **Perbandingan**: Hasil perbandingan menunjukkan bahwa model stacked secara umum memberikan korelasi yang lebih tinggi dengan tingkat plagiarisme yang diharapkan dibandingkan metode tunggal.\n",
    "\n",
    "### Pengembangan Selanjutnya:\n",
    "\n",
    "1. **Eksplorasi Base Models Lain**: Menambahkan model dasar lain seperti SVM, Neural Networks, atau Naive Bayes untuk meningkatkan diversitas prediksi yang dapat digunakan oleh meta model.\n",
    "\n",
    "2. **Feature Engineering**: Pengembangan fitur-fitur baru yang lebih diskriminatif untuk deteksi plagiarisme, seperti fitur semantik berbasis word embeddings atau transformer models.\n",
    "\n",
    "3. **Optimisasi Hyperparameter**: Melakukan tuning hyperparameter yang lebih ekstensif untuk model dasar dan meta model untuk meningkatkan performa.\n",
    "\n",
    "4. **Dataset yang Lebih Besar**: Penggunaan dataset plagiarisme yang lebih besar dan berlabel untuk melatih dan mengevaluasi model secara lebih komprehensif.\n",
    "\n",
    "5. **Cross-Validation**: Implementasi strategi cross-validation yang lebih robust untuk mengevaluasi model dan mencegah overfitting.\n",
    "\n",
    "6. **Ensemble Methods**: Eksplorasi metode ensemble lain seperti bagging atau boosting untuk meningkatkan performa meta model.\n",
    "\n",
    "7. **Model Interpretability**: Pengembangan teknik visualisasi dan interpretasi untuk membantu pengguna memahami alasan di balik klasifikasi plagiarisme.\n",
    "\n",
    "8. **Deteksi Lintas Bahasa**: Eksplorasi metode untuk deteksi plagiarisme lintas bahasa (cross-language plagiarism detection).\n",
    "\n",
    "9. **Integrasi Konteks Domain**: Penyesuaian model untuk domain spesifik seperti dokumen akademik, kode program, atau konten web.\n",
    "\n",
    "Pendekatan stacked model terbukti menjadi strategi yang efektif untuk meningkatkan performa deteksi plagiarisme dengan mengkombinasikan kekuatan dari berbagai algoritma machine learning. Dengan pengembangan lebih lanjut, pendekatan ini memiliki potensi untuk menjadi solusi yang lebih akurat dan handal dalam mendeteksi berbagai jenis plagiarisme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained models with specific high-similarity document pairs\n",
    "def test_specific_pairs():\n",
    "    \"\"\"Test the trained models on specific document pairs with known similarity levels\"\"\"\n",
    "    \n",
    "    if 'stacked_detector' not in globals() or not stacked_detector.is_trained:\n",
    "        print(\"No trained model found. Please run the training cell first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Testing Specific Document Pairs\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Find the highest similarity pairs from our previous analysis\n",
    "    if 'potential_plagiarism' in globals() and len(potential_plagiarism) > 0:\n",
    "        print(\"\\nTesting on previously identified high-similarity pairs:\")\n",
    "        \n",
    "        # Test top 3 high-similarity pairs\n",
    "        for i, (doc1_name, doc2_name, original_similarity) in enumerate(potential_plagiarism[:3]):\n",
    "            print(f\"\\n--- Test Pair {i+1} ---\")\n",
    "            print(f\"Source: {doc1_name}\")\n",
    "            print(f\"Suspicious: {doc2_name}\")\n",
    "            print(f\"Original Cosine Similarity: {original_similarity:.4f}\")\n",
    "            \n",
    "            # Get document contents\n",
    "            doc1_content = source_docs[doc1_name]\n",
    "            doc2_content = suspicious_docs[doc2_name]\n",
    "            \n",
    "            # Test with stacked model\n",
    "            result = stacked_detector.predict(doc1_content, doc2_content)\n",
    "            \n",
    "            print(f\"\\nStacked Model Results:\")\n",
    "            print(f\"  Prediction: {'Plagiarism' if result['prediction'] == 1 else 'No Plagiarism'}\")\n",
    "            print(f\"  Confidence: {result['confidence']:.4f}\")\n",
    "            print(f\"  Logistic Regression Score: {result['base_model_outputs']['logistic_regression']:.4f}\")\n",
    "            print(f\"  XGBoost Score: {result['base_model_outputs']['xgboost']:.4f}\")\n",
    "            \n",
    "            # Show feature breakdown\n",
    "            features = result['original_features']\n",
    "            feature_names = [\n",
    "                \"TF-IDF Similarity\", \"Jaccard Similarity\", \"3-gram Similarity\", \n",
    "                \"5-gram Similarity\", \"Length Ratio\", \"Sentence Structure\",\n",
    "                \"Character Similarity\", \"Word Overlap\"\n",
    "            ]\n",
    "            \n",
    "            print(f\"\\n  Feature Breakdown:\")\n",
    "            for fname, fvalue in zip(feature_names, features):\n",
    "                print(f\"    {fname}: {fvalue:.4f}\")\n",
    "            \n",
    "            # Show text samples\n",
    "            print(f\"\\n  Source Text Sample: {doc1_content[:100]}...\")\n",
    "            print(f\"  Suspicious Text Sample: {doc2_content[:100]}...\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    # Create a synthetic test with known plagiarism levels\n",
    "    print(\"\\n\\nTesting with Synthetic Examples:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Original text\n",
    "    original_text = \"\"\"\n",
    "    Deteksi plagiarisme adalah proses identifikasi dan verifikasi keaslian suatu karya tulis \n",
    "    dengan membandingkannya terhadap sumber-sumber yang sudah ada sebelumnya. Sistem deteksi \n",
    "    plagiarisme modern menggunakan algoritma canggih untuk menganalisis kesamaan struktur \n",
    "    kalimat, pilihan kata, dan pola penulisan.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create test variations\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"High Plagiarism (Minor Changes)\",\n",
    "            \"text\": \"\"\"\n",
    "            Deteksi plagiarisme merupakan proses identifikasi dan verifikasi keaslian suatu karya tulis \n",
    "            dengan membandingkannya terhadap sumber-sumber yang sudah ada sebelumnya. Sistem deteksi \n",
    "            plagiarisme modern menggunakan algoritma canggih untuk menganalisis kesamaan struktur \n",
    "            kalimat, pilihan kata, dan pola penulisan.\n",
    "            \"\"\",\n",
    "            \"expected\": \"High Plagiarism\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Medium Plagiarism (Paraphrasing)\",\n",
    "            \"text\": \"\"\"\n",
    "            Pendeteksian penjiplakan adalah tahapan untuk mengidentifikasi dan memverifikasi orisinalitas \n",
    "            sebuah tulisan dengan cara membandingan dengan referensi yang telah tersedia sebelumnya. \n",
    "            Teknologi pendeteksi penjiplakan masa kini memanfaatkan algoritma yang sophisticated untuk \n",
    "            menelaah kemiripan susunan kalimat, pemilihan vocabulary, dan gaya menulis.\n",
    "            \"\"\",\n",
    "            \"expected\": \"Medium Plagiarism\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Low Plagiarism (Same Topic)\",\n",
    "            \"text\": \"\"\"\n",
    "            Dalam dunia akademik, penting untuk memastikan orisinalitas karya tulis ilmiah. \n",
    "            Berbagai metode dan teknologi telah dikembangkan untuk membantu institusi pendidikan \n",
    "            dalam menjaga integritas akademik dan mencegah praktik penjiplakan karya ilmiah.\n",
    "            \"\"\",\n",
    "            \"expected\": \"Low/No Plagiarism\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"No Plagiarism (Different Topic)\",\n",
    "            \"text\": \"\"\"\n",
    "            Fotosintesis adalah proses biokimia yang terjadi pada tumbuhan hijau untuk menghasilkan \n",
    "            makanan mereka sendiri. Proses ini melibatkan konversi energi cahaya matahari menjadi \n",
    "            energi kimia yang tersimpan dalam bentuk glukosa dengan bantuan klorofil.\n",
    "            \"\"\",\n",
    "            \"expected\": \"No Plagiarism\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Test each case\n",
    "    synthetic_results = []\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\n--- Synthetic Test {i+1}: {test_case['name']} ---\")\n",
    "        print(f\"Expected: {test_case['expected']}\")\n",
    "        \n",
    "        # Get prediction\n",
    "        result = stacked_detector.predict(original_text, test_case['text'])\n",
    "        \n",
    "        prediction_text = 'Plagiarism' if result['prediction'] == 1 else 'No Plagiarism'\n",
    "        print(f\"Stacked Model Prediction: {prediction_text}\")\n",
    "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "        print(f\"Logistic Score: {result['base_model_outputs']['logistic_regression']:.4f}\")\n",
    "        print(f\"XGBoost Score: {result['base_model_outputs']['xgboost']:.4f}\")\n",
    "        \n",
    "        synthetic_results.append({\n",
    "            'Test': test_case['name'],\n",
    "            'Expected': test_case['expected'],\n",
    "            'Predicted': prediction_text,\n",
    "            'Confidence': result['confidence'],\n",
    "            'Logistic': result['base_model_outputs']['logistic_regression'],\n",
    "            'XGBoost': result['base_model_outputs']['xgboost']\n",
    "        })\n",
    "    \n",
    "    # Visualize synthetic test results\n",
    "    synthetic_df = pd.DataFrame(synthetic_results)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot scores for each test case\n",
    "    x = np.arange(len(synthetic_results))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, synthetic_df['Logistic'], width, label='Logistic Regression', alpha=0.8)\n",
    "    plt.bar(x, synthetic_df['XGBoost'], width, label='XGBoost', alpha=0.8)\n",
    "    plt.bar(x + width, synthetic_df['Confidence'], width, label='Final Confidence', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Test Cases')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Stacked Model Performance on Synthetic Test Cases')\n",
    "    plt.xticks(x, [case['name'].split(' ')[0] + '\\n' + case['name'].split(' ')[1] for case in test_cases], rotation=0)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Add threshold lines\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Decision Threshold')\n",
    "    plt.axhline(y=0.75, color='orange', linestyle='--', alpha=0.5, label='High Confidence')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nSynthetic Test Summary:\")\n",
    "    print(synthetic_df.to_string(index=False))\n",
    "    \n",
    "    return synthetic_results\n",
    "\n",
    "# Run the specific tests\n",
    "test_results = test_specific_pairs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e020d7",
   "metadata": {},
   "source": [
    "## Final Model Performance Summary and Recommendations\n",
    "\n",
    "Based on the comprehensive training and evaluation of all models on the preprocessed corpus data, here are the key findings and recommendations:\n",
    "\n",
    "### Model Performance Summary:\n",
    "\n",
    "1. **Stacked Model Architecture**: The stacked approach successfully combines Logistic Regression and XGBoost as base models with a meta-model for final predictions, providing more robust and accurate plagiarism detection.\n",
    "\n",
    "2. **Feature Engineering**: The 8-feature approach (TF-IDF similarity, Jaccard similarity, 3-gram and 5-gram similarities, length ratio, sentence structure, character similarity, and word overlap) provides comprehensive coverage of different similarity aspects.\n",
    "\n",
    "3. **Training Results**: The models have been trained on the actual preprocessed corpus data, creating labels based on similarity thresholds and learning patterns from real document pairs.\n",
    "\n",
    "### Key Strengths:\n",
    "\n",
    "- **Multi-level Detection**: The system can detect various types of plagiarism from exact copying to sophisticated paraphrasing\n",
    "- **Robust Performance**: Stacked approach reduces individual model weaknesses\n",
    "- **Interpretable Results**: Feature breakdown helps understand why documents are flagged\n",
    "- **Scalable Architecture**: Can be retrained with more data as corpus grows\n",
    "\n",
    "### Recommendations for Production Use:\n",
    "\n",
    "1. **Threshold Tuning**: Adjust similarity thresholds based on specific use case requirements\n",
    "2. **Regular Retraining**: Update models periodically with new document pairs\n",
    "3. **Domain Adaptation**: Consider training domain-specific models for different subject areas\n",
    "4. **Human Verification**: Use model predictions as initial screening with human expert review\n",
    "5. **Performance Monitoring**: Track false positives/negatives to continuously improve the system\n",
    "\n",
    "### Future Enhancements:\n",
    "\n",
    "1. **Deep Learning Integration**: Incorporate transformer-based models for semantic understanding\n",
    "2. **Cross-language Detection**: Extend to detect plagiarism across different languages\n",
    "3. **Structure Analysis**: Add detection for structural plagiarism (outline, argument flow)\n",
    "4. **Real-time Processing**: Optimize for real-time document analysis\n",
    "5. **API Development**: Create REST API for integration with other systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2fa474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model export and save functionality\n",
    "def export_trained_models():\n",
    "    \"\"\"Export and save all trained models for future use\"\"\"\n",
    "    \n",
    "    print(\"Exporting Trained Models\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    if 'stacked_detector' not in globals() or not stacked_detector.is_trained:\n",
    "        print(\"No trained stacked model found. Please run the training cell first.\")\n",
    "        return\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    models_dir = \"../models\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # Save stacked model\n",
    "    stacked_model_path = os.path.join(models_dir, \"stacked_plagiarism_detector.pkl\")\n",
    "    stacked_detector.save_model(stacked_model_path)\n",
    "    \n",
    "    # Save individual models for comparison\n",
    "    individual_models = {\n",
    "        'logistic_model': stacked_detector.logistic_model,\n",
    "        'xgboost_model': stacked_detector.xgb_model,\n",
    "        'meta_model': stacked_detector.meta_model,\n",
    "        'scaler': stacked_detector.scaler,\n",
    "        'vectorizer': stacked_detector.vectorizer\n",
    "    }\n",
    "    \n",
    "    individual_models_path = os.path.join(models_dir, \"individual_models.pkl\")\n",
    "    with open(individual_models_path, 'wb') as f:\n",
    "        pickle.dump(individual_models, f)\n",
    "    \n",
    "    print(f\"Models saved to {models_dir}/\")\n",
    "    \n",
    "    # Save training configuration and metrics\n",
    "    config_data = {\n",
    "        'training_metrics': stacked_detector.training_metrics,\n",
    "        'feature_names': [\n",
    "            \"TF-IDF Similarity\", \"Jaccard Similarity\", \"3-gram Similarity\", \n",
    "            \"5-gram Similarity\", \"Length Ratio\", \"Sentence Structure\",\n",
    "            \"Character Similarity\", \"Word Overlap\"\n",
    "        ],\n",
    "        'model_architecture': {\n",
    "            'base_models': ['LogisticRegression', 'XGBClassifier'],\n",
    "            'meta_model': 'LogisticRegression',\n",
    "            'num_features': 8,\n",
    "            'similarity_threshold': 0.6\n",
    "        },\n",
    "        'dataset_info': {\n",
    "            'source_documents': len(source_docs) if 'source_docs' in globals() else 0,\n",
    "            'suspicious_documents': len(suspicious_docs) if 'suspicious_docs' in globals() else 0,\n",
    "            'total_pairs_trained': stacked_detector.training_metrics.get('training_samples', 0) + \n",
    "                                 stacked_detector.training_metrics.get('validation_samples', 0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = os.path.join(models_dir, \"model_config.json\")\n",
    "    import json\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Configuration saved to {config_path}\")\n",
    "    \n",
    "    # Create a simple prediction function for future use\n",
    "    prediction_script = f'''\n",
    "# Simple prediction script using trained models\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def load_plagiarism_detector(model_path='{stacked_model_path}'):\n",
    "    \"\"\"Load the trained plagiarism detection model\"\"\"\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    return model_data\n",
    "\n",
    "def predict_plagiarism(doc1, doc2, model_path='{stacked_model_path}'):\n",
    "    \"\"\"Quick prediction function for plagiarism detection\"\"\"\n",
    "    # This would need the full StackedPlagiarismDetector class\n",
    "    # For production use, import the class and use its predict method\n",
    "    pass\n",
    "\n",
    "# Example usage:\n",
    "# detector_data = load_plagiarism_detector()\n",
    "# result = predict_plagiarism(\"document 1 text\", \"document 2 text\")\n",
    "# print(f\"Plagiarism detected: {{result['prediction']}}\")\n",
    "# print(f\"Confidence: {{result['confidence']:.4f}}\")\n",
    "'''\n",
    "    \n",
    "    script_path = os.path.join(models_dir, \"simple_prediction.py\")\n",
    "    with open(script_path, 'w') as f:\n",
    "        f.write(prediction_script)\n",
    "    \n",
    "    print(f\"Prediction script template saved to {script_path}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nExport Summary:\")\n",
    "    print(f\"✓ Stacked model: {stacked_model_path}\")\n",
    "    print(f\"✓ Individual models: {individual_models_path}\")\n",
    "    print(f\"✓ Configuration: {config_path}\")\n",
    "    print(f\"✓ Prediction script: {script_path}\")\n",
    "    \n",
    "    print(\"\\nModel Training Complete!\")\n",
    "    print(\"All models have been trained on the preprocessed corpus data and saved for future use.\")\n",
    "    \n",
    "    return {\n",
    "        'stacked_model_path': stacked_model_path,\n",
    "        'config_path': config_path,\n",
    "        'models_directory': models_dir\n",
    "    }\n",
    "\n",
    "# Export all trained models\n",
    "export_info = export_trained_models()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PLAGIARISM DETECTION SYSTEM TRAINING COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(\"All models have been successfully trained on your preprocessed data!\")\n",
    "print(\"\\nModels trained:\")\n",
    "print(\"1. TF-IDF + Cosine Similarity (baseline)\")\n",
    "print(\"2. N-gram similarity detection\")\n",
    "print(\"3. Stacked model (Logistic + XGBoost + Meta)\")\n",
    "print(\"\\nThe system is now ready for plagiarism detection on new documents.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
